# Комментарии по итогам ревью

Максим, привет!

Общий комментарий по твоему ревью с моей стороны - ниже по тексту (удобнее, чем comments.txt).

Мы в рамках чуть различных парадигм:
ты предлагаешь "условно-структурный" подход для выгрузки, где единица выгрузки - временной блок (например, день).
Его суть - вот в этой твоей фразе:
<pre>
Инкрементальный ETL процесс проще всего построить, 
логически понять и мониторить, если он построен на датах. 
Т.е. процесс стоит на расписании и забирает данные за 
определенный временной промежуток
</pre>

Мой подход в данной работе - "асинхронные окна", с решением проблемы прихода 
новых данных из API в произвольное время
* Плюсы "структурного" подхода тобой перечислены - проще всего построить, 
логически понять и мониторить
* Минусы мы понимаем одинаково и ты их также отразил в текcте, ниже я их прокомментировал

Мой подход немного сложнее "привычного" ETL:
* stg и dds (за исключением таблицы фактов)- "слепые": им все равно, что и когда пришло - они просто "правильно разложат **уже пришедшие** данные в реляционной структуре" - это общий подход, когда хочется формализовать данные из **не жестко синхронизированных между собой источников**
* dds.fct_deliveries - "точка сбора": в ней мы должны "защититься" для избежания ошибок в логике и эта защита реализована в форме [обработки исключений при неконсистентности (null-значения недопустимы) для pydantic-обьекта в перекладчике](https://github.com/yagrep/de-project-4/blob/main/src/dags/lib/dds_fct_deliveries.py#L76), для случаев, если какие-то данные еще не успели прийти из API (тут все просто: мы останавливем курсор до следующего запуска, возвращая пустой список значений для вставки)
* cdm - аналитика по собранным данным fct: можно запускать в любое время - обновление будет сделано **на основании собранных в fct данных за нужный месяц**

Далее - по тексту, выборочно:

<pre>
1. При заполнении stg слоя мы можем:
...
-к сожалению, в информации о курьерах нет временных отметок. 
В реальной рабочей ситуации следовало бы просто обратиться к разработчикам, 
чтобы добавили поле. В текущем проекте можно либо сделать 
вспомогательную таблицу, либо каждый день перезаписывать данные. 
Не думаю, что количество курьеров может дорасти до такого числа, что перезаписать 
таблицу станет прямо накладно:))
...
</pre>
Последняя мысль - в рамках **предположения** (хотя я с ним, в целом, согласен), но **придется работать с консистентностью явно**,
т.е. реализовать логику, которая точно подтверждает, что для всех доставок на момент построения таблицы фактов
нам точно известны все данные курьеров.   

update_ts, безусловно, решил бы эту проблему.

<pre>
2. При заполнении dds у нас идет перенос данных исключительно в 
SQL. Значит можно обойтись только SQL запросами, не строя сложную логику 
на питоне. Также с помощью контекста Airflow грузить данные за определенный 
временной промежуток (у тебя как раз так сделано для витрины)

Такой ETL процесс будет предельно понятен. Один запуск означает загрузку данных 
за вчерашний день
И его легко мониторить и исправлять. Если вдруг в какой-то из дней была проблема 
с API, то просто в веб-интерфейсе Airflow перезапускаешь даг за эту дату, все 
данные обновляются и становятся актуальными (для этого обязательно использовать 
контекст Airflow для обращения к датам и обеспечивать в коде условие идемпотентности)
</pre>

Тут, как всегда, проблема в подходе. Что лучше: "сделал плавающие асинхронные 
окна и забыл" или "сделал просто и понятно, но нужно периодически поглядывать, 
не было ли проблем с API и перезапускать вручную, если есть проблемы"?. 
Как правило, в любой команде такие моменты согласовываются при проектировании.
Но если у тебя 1000 DAG-ов, за которыми "нужно смотреть и перезапускать из-за 
особенностей источника" - хорошо ли это?

<pre>
3. В спринте вы грузили данные из источников не парся JSON, потому что он был сложной 
структуры и не соответствовал структуре таблицы. В проекте же JSON такой структуры,
что из него можно сразу же получить таблицу, можно делать и так 
(в рекомендации к проекту сказано грузить JSON целиком и это также правильно. 
Ниже я просто предлагаю еще один из возможных вариантов, который может показаться 
тебе удобнее)
</pre>
Здесь - согласен, это действительно проще.

<pre>
4. А точно нужно загрузку stg запускать каждую минуту?))
</pre>
:) Конечно нет, но уж больно долго ждать было (9216 штук) по 50 записей, если не 
раз в минуту до получения витрины. "Догнав наколенное", можно спокойно поменять и на 15 минут -
скорость прихода новых данных, **скорее всего** не потребует большей скорости запросов.

# Комментарии к проекту 5-го спринта
* общая логика слоев - в api_entities.md
* ETL-wrapper  унифицировал под разные виды загрузок с курсором
* Для каждого слоя - отдельный DAG с соответствующим именем
* Работа с асинхронностью загрузки между stg и dds - через "сквозное" использование autoincrement-идентификаторов из stg в dds и использование возвращаемых значений  
* Итоговая витрина - классический execution-date-based DAG, перезписывающий данные за месяц 

# Проект 5-го спринта

### Описание
Репозиторий предназначен для сдачи проекта 5-го спринта

### Как работать с репозиторием
1. В вашем GitHub-аккаунте автоматически создастся репозиторий `de-project-sprint-5` после того, как вы привяжете свой GitHub-аккаунт на Платформе.
2. Скопируйте репозиторий на свой локальный компьютер, в качестве пароля укажите ваш `Access Token` (получить нужно на странице [Personal Access Tokens](https://github.com/settings/tokens)):
	* `git clone https://github.com/{{ username }}/de-project-sprint-5.git`
3. Перейдите в директорию с проектом: 
	* `cd de-project-sprint-5`
4. Выполните проект и сохраните получившийся код в локальном репозитории:
	* `git add .`
	* `git commit -m 'my best commit'`
5. Обновите репозиторий в вашем GutHub-аккаунте:
	* `git push origin main`

### Структура репозитория
- `/src/dags`

### Как запустить контейнер
Запустите локально команду:

```
docker run \
-d \
-p 3000:3000 \
-p 3002:3002 \
-p 15432:5432 \
--mount src=airflow_sp5,target=/opt/airflow \
--mount src=lesson_sp5,target=/lessons \
--mount src=db_sp5,target=/var/lib/postgresql/data \
--name=de-sprint-5-server-local \
sindb/de-pg-cr-af:latest
```

После того как запустится контейнер, вам будут доступны:
- Airflow
	- `localhost:3000/airflow`
- БД
	- `jovyan:jovyan@localhost:15432/de`
